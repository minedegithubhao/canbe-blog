# 微调生成模型

## LLM训练三步走：预训练、监督微调和偏好调优

预训练  → 生成预训练模型（基座）

监督微调（SFT）  →  遵循指令、适应目标任务

偏好调优  →  提高模型质量，使其更符合 AI 安全或人类偏好

## 监督微调（SFT）

- 全量微调：费显卡
- 参数高效微调PEFT：省资源

## 使用QLoRA进行指令微调

> 解决普通人/小显卡训练不起的问题，低成本微调

## 生成评估模型

判断微调后好不好

## 偏好优化、对齐

> 让模型说的对，说的好、说的像人、说的安全

## DPO（偏好优化）

> 解决RLHF太难、太复杂，工业界常用、更稳、更易落地

奖励模型（RM） + 强化学习（RLHF），之恶杰偏好数据训练

## 奖励模型（RM） + 强化学习（RLHF）